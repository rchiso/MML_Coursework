{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Energy consumption & CO2 emission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CodeCarbon output CSV file\n",
    "output_dir = Path(\"./Results/Sustainability\")\n",
    "df = pd.read_csv(output_dir / \"cc_emissions.csv\")\n",
    "\n",
    "# Rename columns \n",
    "# cc_ for CodeCarbon results\n",
    "# ct_ for CarbonTracker results\n",
    "# mlco2_ for MLCO2 Impact results\n",
    "df.rename(columns={\n",
    "    \"duration\": \"t\",\n",
    "    \"emissions\": \"cc_co2\",\n",
    "    \"energy_consumed\": \"cc_energy\"\n",
    "}, inplace=True)\n",
    "\n",
    "dimensions = [9, 12, 15]\n",
    "full_data = df[[\"t\", \"cc_co2\", \"cc_energy\"]].copy()\n",
    "full_data[\"N\"] = dimensions\n",
    "full_data = full_data[[\"N\", \"t\", \"cc_co2\", \"cc_energy\"]]\n",
    "\n",
    "# CarbonTracker data for energy and emissions\n",
    "# Original results are in Results/Sustainability/ct_emissions_n{n}.log file\n",
    "ct_data = {\n",
    "    \"energy\": [0.004169, 0.005497, 0.006868],  # kWh\n",
    "    \"emissions\": [2.005273, 2.644529, 3.303579]  # gCO2eq\n",
    "}\n",
    "\n",
    "# Convert emissions to kgCO2eq\n",
    "ct_data[\"emissions\"] = [em / 1000 for em in ct_data[\"emissions\"]]\n",
    "\n",
    "# MLCO2Impact calculations https://mlco2.github.io/impact/#co2eq\n",
    "T4_power_watt = 70  # Goulge Cloud T4-GPU power in Watts\n",
    "emission_factor_east_asia = 0.56  # kgCO2eq/kWh\n",
    "\n",
    "mlco2_energy = []\n",
    "mlco2_emissions = []\n",
    "\n",
    "for duration in df[\"t\"]:\n",
    "    energy = (duration / 3600) * T4_power_watt / 1000  # Convert to kWh\n",
    "    emission = emission_factor_east_asia * energy\n",
    "\n",
    "    mlco2_energy.append(energy)\n",
    "    mlco2_emissions.append(emission)\n",
    "\n",
    "full_data[\"ct_co2\"] = ct_data[\"emissions\"]\n",
    "full_data[\"ct_energy\"] = ct_data[\"energy\"]\n",
    "full_data[\"mlco2_co2\"] = mlco2_emissions\n",
    "full_data[\"mlco2_energy\"] = mlco2_energy\n",
    "\n",
    "# Print final cleaned data\n",
    "pd.options.display.float_format = \"{:.5f}\".format\n",
    "print(full_data)\n",
    "pd.reset_option(\"display.float_format\")\n",
    "\n",
    "output_file = output_dir / f\"emissions_full.csv\"\n",
    "full_data.to_csv(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = [9,12,15]\n",
    "test_results = {}\n",
    "binary = True\n",
    "\n",
    "for n in dataset_size:\n",
    "    # Load the dataset\n",
    "    X = np.load(\"Datasets/kryptonite-%s-X.npy\"%(n))\n",
    "    y = np.load(\"Datasets/kryptonite-%s-y.npy\"%(n))\n",
    "    \n",
    "    if binary:\n",
    "        X = np.where(X>0.5, 1, 0)\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # 20% test\n",
    "  \n",
    "    X = torch.tensor(X_test.astype(np.float32)).to(device)\n",
    "\n",
    "    para = {\"hidden_size\":[4*n], \"lr\":[0.01], \"alpha\":[0.0001], \"batch_size\":[128], \"num_epochs\":[100]}\n",
    "    model = NeuralNet(n, para[\"hidden_size\"][0]).to(device)\n",
    "\n",
    "    # Load the model\n",
    "    model.load_state_dict(torch.load(\"model/%smodel_best.pth\"%(n)))\n",
    "    model.eval()\n",
    "\n",
    "    output = model(X).cpu().detach().numpy()\n",
    "    output = np.squeeze(output.round()).astype(np.int16)\n",
    "    \n",
    "    columns = [f\"Feature_{i}\" for i in range(n)]\n",
    "    df = pd.DataFrame(X_test, columns=columns)\n",
    "    df[\"Ground_Truth\"] = y_test\n",
    "    df[\"Prediction\"] = output\n",
    "    \n",
    "    test_results[f\"n{n}\"] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in dataset_size:\n",
    "    df = test_results[f\"n{n}\"]\n",
    "    \n",
    "    ground_truth = \"Ground_Truth\" \n",
    "    prediction = \"Prediction\"\n",
    "    features = [f\"Feature_{i}\" for i in range(n)]\n",
    "        \n",
    "    results = []\n",
    "    # Loop through each feature as the protected attribute\n",
    "    for protected_attribute in features:\n",
    "        \n",
    "        binary_dataset = BinaryLabelDataset(\n",
    "            df=df,\n",
    "            label_names=[\"Ground_Truth\"],\n",
    "            protected_attribute_names=[protected_attribute]\n",
    "        )\n",
    "\n",
    "        classified_dataset = binary_dataset.copy()\n",
    "        classified_dataset.labels = df[\"Prediction\"].values.reshape(-1, 1)\n",
    "\n",
    "        classification_metric = ClassificationMetric(\n",
    "            binary_dataset,\n",
    "            classified_dataset,\n",
    "            privileged_groups=[{protected_attribute: 1}],\n",
    "            unprivileged_groups=[{protected_attribute: 0}]\n",
    "        )\n",
    "\n",
    "        # Calculate Metrics\n",
    "        spd = classification_metric.statistical_parity_difference()\n",
    "        eod = classification_metric.equal_opportunity_difference() \n",
    "        di = classification_metric.disparate_impact()\n",
    "        aod = classification_metric.average_odds_difference()\n",
    "        consistency = classification_metric.consistency()[0]\n",
    "\n",
    "        results.append({\n",
    "            \"Feature\": protected_attribute,\n",
    "            \"SPD\": spd,\n",
    "            \"EOD\": eod,\n",
    "            \"DI\": di,\n",
    "            \"AOD\": aod,\n",
    "            \"Consistency\": consistency,\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    save_dir = Path(\"./Results/Sustainability\")\n",
    "    output_file = save_dir / f\"fairness_metrics_n{n}.csv\"\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Fairness metrics for n={n} dataset saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
